name: CI

on:
  push:
    branches: [master]
  pull_request:
    branches: [master]

env:
  LLAMA_CPP_VERSION: b7356
  LLAMA_MODEL_REPO: ggml-org/bge-m3-Q8_0-GGUF
  LLAMA_MODEL_FILE: bge-m3-q8_0.gguf

jobs:
  check:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v6

      - uses: actions/setup-node@v4
        with:
          node-version: 23

      - run: npm install

      - run: npm run build

      - run: npm run check

  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v6

      - uses: actions/setup-node@v4
        with:
          node-version: 23

      - name: Cache llama.cpp
        id: cache-llama
        uses: actions/cache@v4
        with:
          path: ~/.local/llama-cpp
          key: llama-cpp-${{ env.LLAMA_CPP_VERSION }}-ubuntu-x64

      - name: Download llama.cpp prebuilt binary
        if: steps.cache-llama.outputs.cache-hit != 'true'
        run: |
          mkdir -p ~/.local/llama-cpp
          curl -L "https://github.com/ggml-org/llama.cpp/releases/download/${{ env.LLAMA_CPP_VERSION }}/llama-${{ env.LLAMA_CPP_VERSION }}-bin-ubuntu-x64.tar.gz" \
            | tar -xz -C ~/.local/llama-cpp --no-same-owner --strip-components=1

      - name: Add llama.cpp to PATH and LD_LIBRARY_PATH
        run: |
          echo "$HOME/.local/llama-cpp" >> $GITHUB_PATH
          echo "LD_LIBRARY_PATH=$HOME/.local/llama-cpp:$LD_LIBRARY_PATH" >> $GITHUB_ENV

      - name: Cache embedding models
        uses: actions/cache@v4
        with:
          path: ~/.cache/llama.cpp
          key:
            llama-model-${{ env.LLAMA_MODEL_REPO }}-${{ env.LLAMA_MODEL_FILE }}

      - run: npm install

      - run: npm run test
