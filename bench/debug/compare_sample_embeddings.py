#!/usr/bin/env python3
"""
Compare embeddings generated by Python (sentence-transformers) and Transformers.js
"""

import argparse
import json
from pathlib import Path

import numpy as np


def compare_embeddings(python_file: Path, js_file: Path):
    """Compare two embedding files and print detailed analysis."""
    # Load embeddings
    with open(python_file, encoding="utf-8") as f:
        python_data = json.load(f)

    with open(js_file, encoding="utf-8") as f:
        js_data = json.load(f)

    # Use no-norm from Python, regular from JS
    python_emb = np.array(python_data["embedding_no_norm"])
    js_emb = np.array(js_data["embedding"])

    print("=" * 60)
    print("EMBEDDING COMPARISON")
    print("=" * 60)

    # Truncate long text for display
    text = python_data['text']
    max_len = 100
    truncated_text = text if len(text) <= max_len else f"{text[:max_len]}..."
    print(f"\nText: {truncated_text}")
    print(f"Dimension: {len(python_emb)}")

    # Basic statistics
    print("\n--- Python (sentence-transformers) ---")
    print(f"First 5 values: {python_emb[:5]}")
    print(f"L2 norm: {np.linalg.norm(python_emb):.6f}")
    print(f"Min: {python_emb.min():.6f}")
    print(f"Max: {python_emb.max():.6f}")
    print(f"Mean: {python_emb.mean():.6f}")
    print(f"Std: {python_emb.std():.6f}")

    print("\n--- Transformers.js ---")
    print(f"First 5 values: {js_emb[:5]}")
    print(f"L2 norm: {np.linalg.norm(js_emb):.6f}")
    print(f"Min: {js_emb.min():.6f}")
    print(f"Max: {js_emb.max():.6f}")
    print(f"Mean: {js_emb.mean():.6f}")
    print(f"Std: {js_emb.std():.6f}")

    # Normalize both and compare
    python_norm = python_emb / np.linalg.norm(python_emb)
    js_norm = js_emb / np.linalg.norm(js_emb)

    print("\n--- After L2 Normalization ---")
    print(f"Python normalized - First 5: {python_norm[:5]}")
    print(f"JS normalized - First 5: {js_norm[:5]}")

    cosine_sim = np.dot(python_norm, js_norm)
    print(f"\nCosine similarity: {cosine_sim:.6f}")

    # Element-wise difference after normalization
    diff_norm = np.abs(python_norm - js_norm)
    print(f"\nNormalized difference - Mean: {diff_norm.mean():.6f}")
    print(f"\nNormalized difference - Max: {diff_norm.max():.6f}")
    print(f"Normalized difference - Std: {diff_norm.std():.6f}")

    print("\n" + "=" * 60)
    print("CONCLUSION")
    print("=" * 60)

    if cosine_sim > 0.99:
        print("✓ Embeddings are EQUIVALENT")
        print("  Cosine similarity > 0.99 indicates identical embeddings.")
    elif cosine_sim > 0.9:
        print("⚠ Embeddings are SIMILAR but not identical")
        print("  Minor numerical differences detected.")
    else:
        print("✗ Embeddings are FUNDAMENTALLY DIFFERENT")
        print("  This indicates a serious implementation difference!")

    print(f"\nCosine similarity: {cosine_sim:.6f}")

    return cosine_sim


def main():
    parser = argparse.ArgumentParser(
        description="Compare Python and Transformers.js embeddings"
    )
    parser.add_argument(
        "--input-dir",
        type=str,
        default=None,
        help=(
            "Input directory containing embedding files. "
            "If not provided, uses bench/debug/results/"
        ),
    )

    args = parser.parse_args()

    # Resolve paths
    script_dir = Path(__file__).parent.resolve()

    if args.input_dir:
        input_dir = Path(args.input_dir)
    else:
        input_dir = script_dir / "samples"

    # Find all Python embedding files
    python_files = sorted(input_dir.glob("python_embedding_*.json"))

    if not python_files:
        print(f"Error: No Python embedding files found in {input_dir}")
        print("\nRun generate_sample_embeddings.py first")
        return

    # Compare each pair
    compared_count = 0
    results = []  # (identifier, cosine_sim)

    for python_file in python_files:
        # Extract identifier (filename stem without python_embedding_ prefix)
        identifier = python_file.stem.replace("python_embedding_", "")
        js_file = input_dir / f"transformersjs_embedding_{identifier}.json"

        if not js_file.exists():
            print(f"Warning: No matching JS embedding for {python_file.name}")
            print(f"  Expected: {js_file.name}")
            continue

        if compared_count > 0:
            print("\n\n")

        print(f"Comparing: {identifier}")
        print(f"  Python: {python_file}")
        print(f"  JS:     {js_file}")
        print()

        cosine_sim = compare_embeddings(python_file, js_file)
        results.append((identifier, cosine_sim))
        compared_count += 1

    if compared_count == 0:
        print("\nError: No matching embedding pairs found")
        print(
            "\nRun 'Debug: Generate sample embeddings' command in Obsidian "
            "to generate Transformers.js embeddings"
        )
    else:
        print(f"\n\n{'=' * 60}")
        print("FINAL SUMMARY")
        print("=" * 60)
        print(f"Compared {compared_count} embedding pair(s)\n")

        # Check if all are equivalent
        non_equivalent = [(name, sim) for name, sim in results if sim <= 0.99]

        if not non_equivalent:
            print("✓ ALL EMBEDDINGS ARE EQUIVALENT")
            print("  All cosine similarities > 0.99")
        else:
            print(f"✗ {len(non_equivalent)} NON-EQUIVALENT EMBEDDING(S) FOUND:")
            for name, sim in non_equivalent:
                print(f"  - {name}: cosine similarity = {sim:.6f}")
            equiv_count = len(results) - len(non_equivalent)
            print(f"\n  ({equiv_count}/{len(results)} are equivalent)")
        print("=" * 60)


if __name__ == "__main__":
    main()
