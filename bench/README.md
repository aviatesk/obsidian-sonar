# Retrieval benchmark suite

Benchmark comparing Sonar (Obsidian plugin) against Elasticsearch and Weaviate
on multilingual and long document retrieval tasks.

## Overview

This benchmark suite evaluates search quality using two datasets:

**[MIRACL](https://project-miracl.github.io/)**:

- **Task**: Short-text Wikipedia retrieval
- **Languages**: Japanese (~7M docs), English (~33M docs, capped at 7M)
- **Queries**: 860 (ja), 799 (en)
- **Reference**: [MIRACL Dataset](https://project-miracl.github.io/) |
  [Paper](https://arxiv.org/abs/2210.09984)

**[SCIDOCS](https://github.com/beir-cellar/beir)**:

- **Task**: Scientific paper retrieval
- **Corpus**: Paper abstracts (~25K documents)
- **Queries**: 1,000 paper queries (title + abstract)
- **Reference**: [BEIR Benchmark](https://github.com/beir-cellar/beir) |
  [Paper](https://arxiv.org/abs/2104.08663)

Each dataset can be benchmarked with the following search methods across
multiple backends ([Elasticsearch](https://www.elastic.co/elasticsearch/),
[Weaviate](https://weaviate.io/), and Sonar).

- **BM25**: Full-text keyword search
- **Vector**: Dense embedding similarity (requires embeddings)
- **Hybrid**: Combined BM25 + vector with RRF fusion

For Sonar benchmarking, to ensure proper plugin functionality, we convert the corpus 
constructed from the above datasets into markdown files to create a vault, and then 
benchmark by actually enabling Sonar within that vault. Sonar currently uses 
[Transformers.js](https://huggingface.co/docs/transformers.js/en/index) as the backend 
for embedding generation, and this benchmark primarily utilizes embeddings generated 
with that backend.

Benchmarks using Elasticsearch and Weaviate as backends are implemented for comparative validation.
These benchmarks directly load files such as jsonl extracted from the above datasets via CLI,
generate embeddings using [SentenceTransformer](https://huggingface.co/sentence-transformers),
and perform benchmarking by passing that data to those search backends. Docker is used to 
launch these search backends.

The benchmarks for Sonar, Elasticsearch, and Weaviate are implemented to be as fair as possible.
However, due to implementation constraints and for comparative reference, the following differences exist:
- Elasticsearch uses [Kuromoji](https://www.elastic.co/docs/reference/elasticsearch/plugins/analysis-kuromoji-tokenizer) for BM25, so it is expected to achieve better accuracy in BM25 benchmarks that include Japanese documents
- The Elasticsearch/Weaviate implementations use BYO embeddings generated with SentenceTransformer.
  In contrast, Sonar generates embeddings using Transformers.js and manages them in IndexedDB, but
  when using GPU, the embeddings generated by Transformers.js showed significant numerical instability.
  Through this benchmark, I was working to resolve these numerical instabilities on the Sonar side, but there may still be some minor instabilities.

## Transformers.js issues found during benchmarking Sonar

## Invalid embeddings on WebGPU

When using `Xenova/multilingual-e5-base` with WebGPU, different embeddings were generated compared to those generated using WASM as the backend.
Since the similarity between embeddings generated by SentenceTransformer and those generated with the WASM backend was >0.99, it appears that the embeddings generated by the WebGPU backend were invalid. Similar issues:

- https://github.com/huggingface/transformers.js/issues/1046
- https://github.com/microsoft/onnxruntime/issues/24442

**Workaround**: Use `Xenova/multilingual-e5-small` instead.

### NaN embeddings at batch boundaries

**Symptom**: Transformers.js (WASM backend) consistently generates all-NaN embeddings (all 384 dimensions) for the **last item in any batch**, regardless of batch size.

**Discovery**: During MIRACL benchmark (`miracl-ja-en_query-20`, 3455 files, batch_size=32), 7 chunks (0.2%) had completely NaN embeddings. The same texts processed with Python SentenceTransformer produced valid embeddings (norm=1.0, no NaN).

**Impact on search quality**:

Before fix (NaN embeddings present):

| Run                  | nDCG@10 | Recall@10 | MRR@10 | MAP    |
| -------------------- | ------- | --------- | ------ | ------ |
| elasticsearch.vector | 0.9261  | 0.9677    | 0.9373 | 0.8972 |
| weaviate.vector      | 0.9264  | 0.9664    | 0.9385 | 0.8980 |
| **sonar.vector**     | **0.4189** | **0.4694** | **0.5344** | **0.3802** |
| sonar.hybrid         | 0.7643  | 0.8589    | 0.7756 | 0.7214 |

NaN embeddings cause JavaScript `Array.sort((a, b) => b.score - a.score)` to break (NaN comparison returns NaN), corrupting the entire ranking. Example: a document with score 0.921780 (expected rank #1) was placed at rank #1512, while documents with score ~0.75 ranked higher.

**Workaround**: Application-layer NaN detection and skipping during indexing. See `experiments/transformers-js-batch-end-nan.md` for detailed reproduction and analysis.

**Related upstream issue?**: https://github.com/microsoft/onnxruntime/issues/26367

## Results

## Experimental results

This section documents benchmark results comparing different backends and search
methods.

### Results: MIRACL (Japanese + English)

> - Queries: 200
> - Documents: 24754 (JA:EN = 1:1)
> - Embedding model: [`intfloat/multilingual-e5-small`](https://huggingface.co/intfloat/multilingual-e5-small)

| Backend       | Method | nDCG@10 | Recall@10 | Recall@100 | MRR@10 | MAP    |
| ------------- | ------ | ------- | --------- | ---------- | ------ | ------ |
| Sonar         | BM25   | 0.7705  | 0.8700    | 0.9712     | 0.7775 | 0.7210 |
| Sonar         | Vector | 0.9307  | 0.9687    | 0.9912     | 0.9432 | 0.9061 |
| Sonar         | Hybrid | 0.8892  | 0.9444    | 0.9938     | 0.8932 | 0.8580 |
| Elasticsearch | BM25   | 0.7971  | 0.8833    | 0.9530     | 0.8048 | 0.7526 |
| Elasticsearch | Vector | 0.9360  | 0.9759    | 0.9975     | 0.9437 | 0.9127 |
| Elasticsearch | Hybrid | 0.8896  | 0.9348    | 1.0000     | 0.8924 | 0.8630 |
| Weaviate      | BM25   | 0.7467  | 0.8526    | 0.9480     | 0.7473 | 0.6939 |
| Weaviate      | Vector | 0.9358  | 0.9759    | 0.9975     | 0.9432 | 0.9124 |
| Weaviate      | Hybrid | 0.8745  | 0.9316    | 1.0000     | 0.8816 | 0.8464 |

### Results: SCIDOCS

> - Queries: 100
> - Documents: 12426
> - Embedding model: [`intfloat/multilingual-e5-small`](https://huggingface.co/intfloat/multilingual-e5-small)

| Backend       | Method | nDCG@10 | Recall@10 | Recall@100 | MRR@10 | MAP    |
| ------------- | ------ | ------- | --------- | ---------- | ------ | ------ |
| Sonar         | BM25   | 0.1577  | 0.1670    | 0.3475     | 0.2753 | 0.1055 |
| Sonar         | Vector | 0.1659  | 0.1765    | 0.3660     | 0.2888 | 0.1092 |
| Sonar         | Hybrid | 0.1830  | 0.1960    | 0.3965     | 0.3043 | 0.1234 |
| Elasticsearch | BM25   | 0.1568  | 0.1660    | 0.3450     | 0.2620 | 0.1075 |
| Elasticsearch | Vector | 0.1506  | 0.1665    | 0.3600     | 0.2587 | 0.0977 |
| Elasticsearch | Hybrid | 0.1792  | 0.1865    | 0.3780     | 0.3078 | 0.1239 |
| Weaviate      | BM25   | 0.1482  | 0.1530    | 0.3370     | 0.2512 | 0.1044 |
| Weaviate      | Vector | 0.1562  | 0.1685    | 0.3580     | 0.2750 | 0.1013 |
| Weaviate      | Hybrid | 0.1799  | 0.1905    | 0.3805     | 0.3053 | 0.1231 |

### Accuracy characteristics

TODO

### Notes

#### Metric definitions

- `nDCG@10`:
  [Normalized Discounted Cumulative Gain](https://en.wikipedia.org/wiki/Discounted_cumulative_gain)
  at rank 10. Evaluates ranking quality by considering both relevance and
  position. Higher scores indicate more relevant documents appear at higher
  ranks. Range: 0-1, where 1.0 is ideal.
- `Recall@10`: Proportion of relevant docs found in top 10 results. Measures how
  well the system captures relevant documents in the initial results. Higher
  values mean fewer relevant documents are missed in the top 10.
- `Recall@100`: Proportion of relevant docs found in top 100 results. Measures
  overall retrieval coverage. Important for applications where users browse
  multiple pages of results.
- `MRR@10`:
  [Mean Reciprocal Rank](https://en.wikipedia.org/wiki/Mean_reciprocal_rank) at
  cutoff 10. Measures how quickly users find the first relevant result. Score is
  1.0 if the first relevant document is at rank 1, 0.5 at rank 2, 0.333 at rank
  3, etc. Averaged across all queries. Higher scores mean users find relevant
  results faster.
- `MAP`:
  [Mean Average Precision](<https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision>)
  across all queries. A comprehensive metric that balances precision and recall
  across all rank positions. Particularly sensitive to the ranking of all
  relevant documents, not just the first one.

For detailed metric explanations, see
[ir-measures documentation](https://ir-measur.es/en/latest/measures.html).

> [!TIP] **nDCG@10 vs MAP**: Both are comprehensive metrics, but serve different
> purposes:
>
> - **nDCG@10**: Measures "quality of the first screen" (top 10 results). Uses
>   logarithmic discounting, so rank 1 vs 2 matters much more than rank 9 vs 10.
>   Focuses on what users immediately see. Best for evaluating user-facing
>   search quality.
> - **MAP**: Measures "overall system ranking ability" across all relevant
>   documents. Treats the 1st and 100th relevant document equally. No cutoff,
>   evaluates complete ranking. Best for evaluating comprehensive retrieval
>   performance.
>
> In practice: High nDCG@10 with lower MAP means good initial results but misses
> some relevant docs deeper in ranking. High MAP with lower nDCG@10 means good
> overall coverage but top results could be better ordered.

## Benchmark workflow

### Step 0: Prerequisites

- **Python dependencies**: Install [uv](https://docs.astral.sh/uv/)
- **Docker**: Required for running Elasticsearch and Weaviate
  - macOS: `brew install --cask docker`
  - Or download from <https://www.docker.com/products/docker-desktop/>

> [!NOTE] All commands below should be run from this directory (`/bench`).

### Step 1: Install Python dependencies

```bash
uv sync
```

### Step 2: Download datasets

#### MIRACL (Multilingual retrieval)

Download Japanese and English Wikipedia datasets:

```bash
uv run scripts/download_datasets.py --datasets miracl_ja,miracl_en --splits dev
```

Output: `datasets/raw/miracl_{ja,en}_*_dev.{jsonl,tsv}`

#### SCIDOCS (Scientific papers)

Download scientific paper dataset:

```bash
uv run scripts/download_datasets.py --datasets scidocs
```

Output: `datasets/raw/scidocs_*.{jsonl,tsv}`

### Step 3: Generate benchmark subset

#### MIRACL subset

Generate a mixed Japanese/English subset with 200 queries (100 ja + 100 en):

```bash
uv run scripts/generate_subset.py \
  --corpus datasets/raw/miracl_ja_corpus_dev.jsonl,datasets/raw/miracl_en_corpus_dev.jsonl \
  --queries datasets/raw/miracl_ja_queries_dev.jsonl,datasets/raw/miracl_en_queries_dev.jsonl \
  --qrels datasets/raw/miracl_ja_qrels_dev.tsv,datasets/raw/miracl_en_qrels_dev.tsv \
  --n-queries 200 \
  --output-dir datasets/processed/miracl-ja-en_query-200
```

This creates:

- `datasets/processed/miracl-ja-en_query-200/corpus.jsonl` -
  Document corpus (~18,000 documents)
- `datasets/processed/miracl-ja-en_query-200/queries.jsonl` - Test
  queries (200 queries)
- `datasets/processed/miracl-ja-en_query-200/qrels.tsv` - Relevance
  judgments

For multiple datasets, queries are sampled with equal distribution (1:1) by
default. To customize the ratio, use `--query-ratio`:

```bash
# Example: 2x more Japanese queries than English (133 ja + 67 en)
uv run scripts/generate_subset.py \
  --corpus datasets/raw/miracl_ja_corpus_dev.jsonl,datasets/raw/miracl_en_corpus_dev.jsonl \
  --queries datasets/raw/miracl_ja_queries_dev.jsonl,datasets/raw/miracl_en_queries_dev.jsonl \
  --qrels datasets/raw/miracl_ja_qrels_dev.tsv,datasets/raw/miracl_en_qrels_dev.tsv \
  --seed 42 \
  --n-queries 200 \
  --query-ratio 2:1 \
  --output-dir datasets/processed/miracl-ja-en_query-200_ja2en1
```

#### SCIDOCS subset

Generate a scientific paper subset with 100 queries:

```bash
uv run scripts/generate_subset.py \
  --corpus datasets/raw/scidocs_corpus.jsonl \
  --queries datasets/raw/scidocs_queries.jsonl \
  --qrels datasets/raw/scidocs_qrels.tsv \
  --seed 42 \
  --n-queries 100  \
  --output-dir datasets/processed/scidocs_query-100
```

This creates:

- `datasets/processed/scidocs_query-100/corpus.jsonl` - Paper abstracts corpus
  (~18,000 documents)
- `datasets/processed/scidocs_query-100/queries.jsonl` - Research queries (100
  queries)
- `datasets/processed/scidocs_query-100/qrels.tsv` - Relevance judgments

### Step 4: Generate embeddings (for vector/hybrid search)

Skip this step if you only want to run BM25 benchmarks.

#### MIRACL embeddings

Generate corpus embeddings:

```bash
uv run scripts/generate_embeddings.py \
  --corpus datasets/processed/miracl-ja-en_query-200/corpus.jsonl \
  --output embeddings/miracl-ja-en_query-200/intfloat/multilingual-e5-small/corpus_embeddings.jsonl \
  --model intfloat/multilingual-e5-small
```

Generate query embeddings:

```bash
uv run scripts/generate_embeddings.py \
  --corpus datasets/processed/miracl-ja-en_query-200/queries.jsonl \
  --output embeddings/miracl-ja-en_query-200/multilingual-e5-small/query_embeddings.jsonl \
  --model intfloat/multilingual-e5-small
```

#### SCIDOCS embeddings

Generate corpus embeddings:

```bash
uv run scripts/generate_embeddings.py \
  --corpus datasets/processed/scidocs_query-100/corpus.jsonl \
  --output embeddings/scidocs_query-100/intfloat/multilingual-e5-small/corpus_embeddings.jsonl \
  --model intfloat/multilingual-e5-small
```

Generate query embeddings:

```bash
uv run scripts/generate_embeddings.py \
  --corpus datasets/processed/scidocs_query-100/queries.jsonl \
  --output embeddings/scidocs_query-100/intfloat/multilingual-e5-small/query_embeddings.jsonl \
  --model intfloat/multilingual-e5-small
```

###### Device selection

- The script automatically detects and uses GPU (CUDA/MPS) if available
- Force CPU usage: add `--device cpu`
- Adjust batch size based on GPU memory (default: 128 for GPU, 32 for CPU)

#### Long document handling

- Documents are automatically chunked if they exceed model's max sequence length
  (default: 512 tokens per chunk, 128 token overlap)
- Each chunk is stored as a separate embedding with metadata (`doc_id`,
  `chunk_index`, `text`)
- Chunk scores are aggregated to document level at search time (see Advanced
  section in Step 5)

#### Model selection

Any sentence-transformers compatible model from Hugging Face can be used.
Examples:

- `intfloat/multilingual-e5-small` (384 dims, ~470MB, default)
- `intfloat/multilingual-e5-base` (768 dims, ~1.1GB, currently has accuracy issues when used with Transformers.js)
- `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` (384 dims)

### Step 4.5: Generate Obsidian vault (Sonar only)

If you plan to benchmark Sonar (Obsidian plugin), generate vault format from the
corpus:

#### MIRACL vault

```bash
uv run scripts/generate_vault.py \
  --corpus datasets/processed/miracl-ja-en_query-200/corpus.jsonl \
  --output vaults/miracl-ja-en_query-200
```

#### SCIDOCS vault

```bash
uv run scripts/generate_vault.py \
  --corpus datasets/processed/scidocs_query-100/corpus.jsonl \
  --output vaults/scidocs_query-100
```

Skip this step if you only want to benchmark Elasticsearch and Weaviate.

### Step 5: Run benchmarks

#### Sonar (Obsidian plugin)

Setup benchmark configuration and run all search methods:

```bash
VAULT=/path/to/the/vault
DATASET=datasets/processed/miracl-ja-en_query-200

mkdir -p $VAULT/.obsidian/plugins/sonar/
cp ../main.js ../manifest.json ../styles.css data.json $VAULT/.obsidian/plugins/sonar/

# Edit data.json to set benchmark paths (use vault-relative or absolute paths)
# Set the following fields:
#   "benchmarkQueriesPath": "/absolute/path/to/bench/datasets/processed/dataset/queries.jsonl"
#   "benchmarkQrelsPath": "/absolute/path/to/bench/datasets/processed/dataset/qrels.tsv"
#   "benchmarkOutputDir": "/absolute/path/to/bench/runs"
# Copy benchmark configuration
cp data.json $VAULT/.obsidian/plugins/sonar/data.json

# 5. Open vault in Obsidian
# 6. Wait for Sonar indexing to complete (check status bar)
# 7. Run benchmark command:
#    - Open Command Palette (Cmd+P / Ctrl+P)
#    - Run "Sonar: Run benchmark (BM25, Vector, Hybrid)"
# 8. Results will be written to:
#    - /path/to/your/vault/runs/sonar.bm25.trec
#    - /path/to/your/vault/runs/sonar.vector.trec
#    - /path/to/your/vault/runs/sonar.hybrid.trec
```

#### Elasticsearch & Weaviate

##### Quick start: automated benchmark

Run the entire benchmark pipeline (Docker startup, indexing, search, evaluation)
with a single script:

```bash
./runbechmark.sh --dataset datasets/processed/miracl-ja-en_query-200
```

This will run all backends (Elasticsearch and Weaviate) with all methods (BM25,
Vector, Hybrid) and output evaluation results.

Options:

```bash
# Use a different dataset
./runbechmark.sh --dataset datasets/processed/scidocs_query-100

# Use a different model for embeddings (need to specify vector dimention depending on model to be used)
./runbechmark.sh --model intfloat/multilingual-e5-small --dataset datasets/processed/scidocs_query-100 --vector-dims 768
```

Use `./runbechmark.sh --help` for full options.

##### Manual benchmark steps

If you prefer to run each step manually:

Start Elasticsearch and Weaviate backends:

```bash
docker compose up -d
```

Verify services are running:

```bash
# Elasticsearch
curl http://localhost:9200

# Weaviate
curl http://localhost:8080/v1/.well-known/ready
```

###### Elasticsearch

BM25 search (keyword-only):

```bash
# Index corpus for BM25
uv run scripts/index.py \
  --backend elasticsearch \
  --dataset datasets/processed/miracl-ja-en_query-200

# Search with BM25
uv run scripts/search.py \
  --backend elasticsearch \
  --queries datasets/processed/miracl-ja-en_query-200/queries.jsonl \
  --output runs/es.bm25.trec \
  --method bm25
```

Vector/hybrid search (requires embeddings from Step 4):

```bash
# Index chunks with embeddings
uv run scripts/index.py \
  --backend elasticsearch \
  --embeddings embeddings/miracl-ja-en_query-200/multilingual-e5-small/corpus_embeddings.jsonl

# Vector search
uv run scripts/search.py \
  --backend elasticsearch \
  --queries datasets/processed/miracl-ja-en_query-200/queries.jsonl \
  --output runs/es.vector.trec \
  --method vector \
  --embeddings embeddings/miracl-ja-en_query-200/multilingual-e5-small/query_embeddings.jsonl

# Hybrid search (BM25 + Vector with RRF fusion)
uv run scripts/search.py \
  --backend elasticsearch \
  --queries datasets/processed/miracl-ja-en_query-200/queries.jsonl \
  --output runs/es.hybrid.trec \
  --method hybrid \
  --embeddings embeddings/miracl-ja-en_query-200/multilingual-e5-small/query_embeddings.jsonl
```

###### Weaviate

BM25 search (keyword-only):

```bash
# Index corpus for BM25
uv run scripts/index.py \
  --backend weaviate \
  --dataset datasets/processed/miracl-ja-en_query-200

# Search with BM25
uv run scripts/search.py \
  --backend weaviate \
  --queries datasets/processed/miracl-ja-en_query-200/queries.jsonl \
  --output runs/weaviate.bm25.trec \
  --method bm25
```

Vector/hybrid search (requires embeddings from Step 4):

```bash
# Index chunks with embeddings
uv run scripts/index.py \
  --backend weaviate \
  --embeddings embeddings/miracl-ja-en_query-200/multilingual-e5-small/corpus_embeddings.jsonl

# Vector search
uv run scripts/search.py \
  --backend weaviate \
  --queries datasets/processed/miracl-ja-en_query-200/queries.jsonl \
  --output runs/weaviate.vector.trec \
  --method vector \
  --embeddings embeddings/miracl-ja-en_query-200/multilingual-e5-small/query_embeddings.jsonl

# Hybrid search (BM25 + Vector with RRF fusion)
uv run scripts/search.py \
  --backend weaviate \
  --queries datasets/processed/miracl-ja-en_query-200/queries.jsonl \
  --output runs/weaviate.hybrid.trec \
  --method hybrid \
  --embeddings embeddings/miracl-ja-en_query-200/multilingual-e5-small/query_embeddings.jsonl
```

##### Advanced: Chunk aggregation parameters

All search methods support chunk-level retrieval with document-level
aggregation:

- `--chunk-top-k`: Number of chunks to retrieve (default: 100)
- `--agg-method`: Aggregation method (default: `max_p`)
  - `max_p`: Maximum score across chunks (MaxP)
  - `top_m_sum`: Sum of top m chunk scores
  - `top_m_avg`: Average of top m chunk scores
  - `rrf_per_doc`: RRF fusion within document chunks
- `--agg-m`: Number of top chunks per document for `top_m_*` methods
  (default: 3)

Example with custom aggregation:

```bash
uv run scripts/search.py \
  --backend elasticsearch \
  --queries datasets/processed/miracl-ja-en_query-200/queries.jsonl \
  --output runs/es.bm25.max_p.trec \
  --method bm25 \
  --chunk-top-k 200 \
  --agg-method max_p
```

### Step 6: Evaluate results

If you used `./runbechmark.sh`, evaluation is already complete. Otherwise,
run:

```bash
uv run scripts/evaluate.py \
  --runs runs/*.trec \
  --qrels datasets/processed/miracl-ja-en_query-200/qrels.tsv
```

### Step 7: Clean up

Stop Docker services:

```bash
docker compose down
```

To also remove indexed data volumes:

```bash
docker compose down -v
```

## Troubleshooting

### Out of memory during subset generation

Large corpora (especially MIRACL-en) can exhaust memory. Limit corpus size:

```bash
uv run scripts/generate_subset.py \
  --corpus datasets/raw/miracl_ja_corpus_dev.jsonl,datasets/raw/miracl_en_corpus_dev.jsonl \
  --queries datasets/raw/miracl_ja_queries_dev.jsonl,datasets/raw/miracl_en_queries_dev.jsonl \
  --qrels datasets/raw/miracl_ja_qrels_dev.tsv,datasets/raw/miracl_en_qrels_dev.tsv \
  --n-queries 200 \
  --max-docs-per-dataset 1000000  # Limit to 1M docs per dataset
```
